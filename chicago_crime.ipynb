{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyspark in c:\\users\\joyce\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (3.2.1)\n",
      "Requirement already satisfied: py4j==0.10.9.3 in c:\\users\\joyce\\appdata\\local\\programs\\python\\python39\\lib\\site-packages (from pyspark) (0.10.9.3)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.functions import year, month, dayofweek, hour\n",
    "\n",
    "\n",
    "\n",
    "ssql = SparkSession.builder.master(\"local\").appName(\"chicago_crime_p1\") .config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "\n",
    "data_schema = StructType([StructField(\"n/a\", StringType()), StructField(\"ID\", StringType()),StructField(\"Case Number\", StringType()), StructField(\"Date\", StringType()),\n",
    "StructField(\"Block\", StringType()), StructField(\"IUCR\", DoubleType()),StructField(\"Primary Type\", StringType()), StructField(\"Description\", StringType()),\n",
    "StructField(\"Location Description\", StringType()), StructField(\"Arrest\", StringType()),StructField(\"Domestic\", StringType()), StructField(\"Beat\", DoubleType()),\n",
    "StructField(\"District\", DoubleType()), StructField(\"Ward\", DoubleType()),StructField(\"Community Area\", DoubleType()), StructField(\"FBI Code\", StringType()),\n",
    "StructField(\"X Coordinate\", DoubleType()), StructField(\"Y Coordinate\", DoubleType()),StructField(\"Year\", StringType()), StructField(\"Updated On\", StringType()),\n",
    "StructField(\"Latitude\", DoubleType()), StructField(\"Longitude\", DoubleType()),StructField(\"Location\", StringType())])\n",
    "timestamp = func.to_timestamp(col(\"Date\"), \"M/d/yyyy H:mm:ss a\")\n",
    "# method 1\n",
    "# dataset =ssql.read.format(\"csv\").option(\"header\",\"true\").schema(data_schema).load(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2001_to_2004.csv\") #withColumn(\"new_date\", to_date(col(\"date\"),\"yyyy.MM.dd.\"))\n",
    "# method 2\n",
    "#read and combine all datasets\n",
    "importedDataset1 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2001_to_2004.csv\", header=\"true\", \n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset2 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2005_to_2007.csv\", header=\"true\",\n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset3 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2008_to_2011.csv\", header=\"true\",\n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset4 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2012_to_2017.csv\", header=\"true\",\n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset = importedDataset1.union(importedDataset2).union(importedDataset3).union(importedDataset4)\n",
    "#format time columns\n",
    "dataset = importedDataset.withColumn(\"timestamp\", timestamp).withColumn(\"Month\", month(col(\"timestamp\"))).withColumn(\"Day of Week\", dayofweek(col(\"timestamp\"))).withColumn(\"Hour\", hour(col(\"timestamp\"))).drop(\"Date\").drop(\"timestamp\").cache()\n",
    "##################################################################\n",
    "#Total crime count in each district, ward, and community area\n",
    "crimeByDistrict = dataset.groupBy(\"District\").agg(func.count(func.lit(1)).alias(\"Total Crime Count\")).filter(\"District is NOT NULL\").sort(func.desc(\"Total Crime Count\"))\n",
    "# crimeByWard = dataset.groupBy(\"Ward\").agg(func.count(func.lit(1)).alias(\"Total Crime Count\")).filter(col(\"Ward\").isNotNull()).sort(func.desc(\"Total Crime Count\")).show()\n",
    "# crimeByCommunity = dataset.groupBy(\"Community Area\").agg(func.count(func.lit(1)).alias(\"Total Crime Count\")).filter(col(\"Community Area\").isNotNull()).sort(func.desc(\"Total Crime Count\")).show()\n",
    "\n",
    "dataset.describe()\n",
    "\n",
    "#Crime count by type in each district\n",
    "crimeByDistrictType = dataset.groupBy(\"District\", \"Primary Type\").agg(func.count(\"Primary Type\").alias(\"Total Crime Count\")).filter((col(\"District\").isNotNull()) & (col(\"Primary Type\")!=\"OTHER OFFENSE\")).orderBy(col(\"District\").asc(), col(\"Total Crime Count\").desc())\n",
    "\n",
    "#Homicide count in each district\n",
    "homicideByDistrict = crimeByDistrictType.filter(col(\"Primary Type\")==\"HOMICIDE\")\n",
    "\n",
    "#Homicide count by month, day of week and hour\n",
    "crimeByYear = dataset.filter(col(\"Primary Type\")==\"HOMICIDE\").groupBy(\"Year\").count().sort(func.desc(\"count\"))\n",
    "crimeByMonth = dataset.filter(col(\"Primary Type\")==\"HOMICIDE\").groupBy(\"Month\").agg(func.count(\"Month\")).orderBy(\"Month\", col(\"count(Month)\").desc())\n",
    "crimeByDay = dataset.filter(col(\"Primary Type\")==\"HOMICIDE\").groupBy(\"Day of Week\").agg(func.count(\"Day of Week\")).orderBy(\"Day of Week\", col(\"count(Day of Week)\").desc())\n",
    "crimeByHour = dataset.filter(col(\"Primary Type\")==\"HOMICIDE\").groupBy(\"Hour\").agg(func.count(\"Hour\")).orderBy(col(\"count(Hour)\").desc())\n",
    "\n",
    "\n",
    "#Crime by location description\n",
    "crimeByLocation = dataset.groupBy(\"Primary Type\").agg(func.collect_set(\"Location Description\"))\n",
    "\n",
    "#Homicide by location description\n",
    "homicideByLocation = dataset.filter(col(\"Primary Type\")==\"HOMICIDE\").select(\"Primary Type\", \"Location Description\").withColumn(\"location\", split(col(\"Location Description\"), \" \")).drop(\"Location Description\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType,StructField, StringType, DoubleType\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import col, split\n",
    "from pyspark.sql.functions import year, month, dayofweek, hour\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import StringIndexer, VectorAssembler\n",
    "from pyspark.ml.feature import OneHotEncoder\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.tuning import ParamGridBuilder, CrossValidator\n",
    "from pyspark.ml.evaluation import BinaryClassificationEvaluator, MulticlassClassificationEvaluator\n",
    "\n",
    "\n",
    "ssql = SparkSession.builder.master(\"local\").appName(\"chicago_crime_p1\") .config(\"spark.some.config.option\", \"some-value\").getOrCreate()\n",
    "\n",
    "\n",
    "# import data from csv into dataframes\n",
    "    # data_schema = \"\"\"\n",
    "    # 'n/a' String,\n",
    "    # 'ID' String,\n",
    "    # 'Case Number' String,\n",
    "    # 'Date' String,\n",
    "    # 'Block' String,\n",
    "    # 'IUCR' String,\n",
    "    # 'Primary Type' String,\n",
    "    # 'Description' String,\n",
    "    # 'Location Description' String,\n",
    "    # 'Arrest' String,\n",
    "    # 'Domestic' String,\n",
    "    # 'Beat' String,\n",
    "    # 'District' String,\n",
    "    # 'Ward' String,\n",
    "    # 'Community Area' String,\n",
    "    # 'FBI Code' String,\n",
    "    # 'X Coordinate' String,\n",
    "    # 'Y Coordinate' String,\n",
    "    # 'Year' String,\n",
    "    # 'Updated On' String,\n",
    "    # 'Latitude' String,\n",
    "    # 'Longitude' String,\n",
    "    # \"Location' String\n",
    "    # \"\"\"\n",
    "\n",
    "# Label: Arrest (True/False)\n",
    "# Feature DoubleTypes: District, Ward, Community Area, Beat, IUCR, Year, Y Coordinate, X Coordinate, Latitude, Longitude\n",
    "\n",
    "\n",
    "data_schema = StructType([StructField(\"n/a\", StringType()), StructField(\"ID\", StringType()),StructField(\"Case Number\", StringType()), StructField(\"Date\", StringType()),\n",
    "StructField(\"Block\", StringType()), StructField(\"IUCR\", DoubleType()),StructField(\"Primary Type\", StringType()), StructField(\"Description\", StringType()),\n",
    "StructField(\"Location Description\", StringType()), StructField(\"Arrest\", StringType()),StructField(\"Domestic\", StringType()), StructField(\"Beat\", DoubleType()),\n",
    "StructField(\"District\", DoubleType()), StructField(\"Ward\", DoubleType()),StructField(\"Community Area\", DoubleType()), StructField(\"FBI Code\", StringType()),\n",
    "StructField(\"X Coordinate\", DoubleType()), StructField(\"Y Coordinate\", DoubleType()),StructField(\"Year\", DoubleType()), StructField(\"Updated On\", StringType()),\n",
    "StructField(\"Latitude\", DoubleType()), StructField(\"Longitude\", DoubleType()),StructField(\"Location\", StringType())])\n",
    "timestamp = func.to_timestamp(col(\"Date\"), \"M/d/yyyy H:mm:ss a\")\n",
    "# method 1\n",
    "# dataset =ssql.read.format(\"csv\").option(\"header\",\"true\").schema(data_schema).load(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2001_to_2004.csv\") #withColumn(\"new_date\", to_date(col(\"date\"),\"yyyy.MM.dd.\"))\n",
    "# method 2\n",
    "importedDataset1 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2001_to_2004.csv\", header=\"true\", \n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset2 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2005_to_2007.csv\", header=\"true\",\n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset3 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2008_to_2011.csv\", header=\"true\",\n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset4 = ssql.read.csv(r\"C:\\Users\\joyce\\Jupyter_Notebooks\\files\\Chicago_Crimes_2012_to_2017.csv\", header=\"true\",\n",
    "schema = data_schema).drop(\"n/a\").drop(\"ID\").drop(\"Case Number\").drop(\"Location\").drop(\"FBI Code\").drop(\"Updated On\").drop('Block').drop('Description').drop(\"X Coordinate\").drop(\"Y Coordinate\")\n",
    "importedDataset = importedDataset1.union(importedDataset2).union(importedDataset3).union(importedDataset4)\n",
    "#format time columns\n",
    "dataset = importedDataset.withColumn(\"timestamp\", timestamp).withColumn(\"Month\", month(col(\"timestamp\"))).withColumn(\"Day of Week\", dayofweek(col(\"timestamp\"))).withColumn(\"Hour\", hour(col(\"timestamp\"))).drop(\"Date\").drop(\"timestamp\").cache()\n",
    "##################################################################\n",
    "\n",
    "# Split dataset into train and test datasets\n",
    "split = dataset.randomSplit([0.8, 0.2], seed=42)\n",
    "trainDF = split[0]\n",
    "testDF = split[1]\n",
    "# print(trainDF.cache().count()) # Cache because accessing training data multiple times\n",
    "# print(testDF.count())\n",
    "\n",
    "\n",
    "categoricalCols = ['Arrest','Location Description','Domestic']\n",
    "\n",
    "# Initialize the pipeline\n",
    "# stages = [] \n",
    " \n",
    "# uses StringIndexer to convert each categorical variable into a series of levels, and then uses OneHotEncoder to convert each categorical column into a set of columns, one for each level.\n",
    "stringIndexer = StringIndexer(inputCols=categoricalCols, outputCols=[categoricalCol + \"Index\" for categoricalCol in categoricalCols])\n",
    "stringIndexer.setHandleInvalid(\"skip\")\n",
    "encoder = OneHotEncoder(inputCols=stringIndexer.getOutputCols(), outputCols=[categoricalCol + \"OHE\" for categoricalCol in categoricalCols])\n",
    " \n",
    "    \n",
    "    \n",
    "labelToIndex = StringIndexer(inputCol=\"Primary Type\", outputCol=\"label\")\n",
    "labelToIndex.setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    " \n",
    "# This includes both the numeric columns and the one-hot encoded binary vector columns in our dataset.\n",
    "numericCols = ['IUCR',\n",
    "'Beat',\n",
    "'District',\n",
    "'Ward',\n",
    "'Community Area',\n",
    "'Latitude',\n",
    "'Longitude', \"Year\", \"Month\", \"Day of Week\", \"Hour\"]\n",
    "assemblerInputs = [c + \"OHE\" for c in categoricalCols] + numericCols\n",
    "vecAssembler = VectorAssembler(inputCols=assemblerInputs, outputCol=\"features\")\n",
    "vecAssembler.setHandleInvalid(\"skip\")\n",
    "\n",
    "\n",
    "lr = LogisticRegression(featuresCol=\"features\", labelCol=\"label\", regParam=1.0)\n",
    "\n",
    "# Define the pipeline based on the stages created in previous steps.\n",
    "pipeline = Pipeline(stages=[stringIndexer, encoder, labelToIndex, vecAssembler, lr])\n",
    " \n",
    "# Define the pipeline model.\n",
    "pipelineModel = pipeline.fit(trainDF)\n",
    "\n",
    "# Examine the coefficient for each variable\n",
    "for col, coef in zip(assemblerInputs, pipelineModel.coefficients):\n",
    "    print(col, coef)\n",
    "print(f\"intercept: {pipelineModel.intercept}\")\n",
    "\n",
    "# Apply the pipeline model to the test dataset.\n",
    "predDF = pipelineModel.transform(testDF)\n",
    "\n",
    "predDF.select(\"features\", \"label\", \"prediction\", \"probability\").show()\n",
    "\n",
    "\n",
    "bcEvaluator = BinaryClassificationEvaluator(metricName=\"areaUnderROC\")\n",
    "print(f\"Area under ROC curve: {bcEvaluator.evaluate(predDF)}\")\n",
    " \n",
    "mcEvaluator = MulticlassClassificationEvaluator(metricName=\"accuracy\")\n",
    "print(f\"Accuracy: {mcEvaluator.evaluate(predDF)}\")\n",
    "\n",
    "paramGrid = (ParamGridBuilder()\n",
    "             .addGrid(lr.regParam, [0.01, 0.5, 2.0])\n",
    "             .addGrid(lr.elasticNetParam, [0.0, 0.5, 1.0])\n",
    "             .build())\n",
    "             \n",
    "# Create a 3-fold CrossValidator\n",
    "cv = CrossValidator(estimator=pipeline, estimatorParamMaps=paramGrid, evaluator=bcEvaluator, numFolds=3, parallelism = 4)\n",
    " \n",
    "# Run cross validations. This step takes a few minutes and returns the best model found from the cross validation.\n",
    "cvModel = cv.fit(trainDF)\n",
    "\n",
    "# Use the model identified by the cross-validation to make predictions on the test dataset\n",
    "cvPredDF = cvModel.transform(testDF)\n",
    " \n",
    "# Evaluate the model's performance based on area under the ROC curve and accuracy \n",
    "print(f\"Area under ROC curve: {bcEvaluator.evaluate(cvPredDF)}\")\n",
    "print(f\"Accuracy: {mcEvaluator.evaluate(cvPredDF)}\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "a3e455487d35a1aac2f353843325f59e1cc138e5a96d9a1cd21bbb1a34a1ca0d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
